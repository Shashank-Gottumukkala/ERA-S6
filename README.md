# Backpropagation
Backpropagation is a key technique used in training neural networks through gradient descent. It involves propagating the error backward through the network to update the weights and biases. 
  - Neural networks consist of interconnected layers of nodes (neurons) that process input data to produce output predictions.
  - During the forward pass, input data is fed through the network, and activations are computed for each neuron using an activation function.
  - The predicted output is compared to the desired output using a loss function.
  - Backpropagation calculates the gradient of the error with respect to each weight and bias in the network.
  - The gradient descent algorithm is then used to update the weights and biases in the opposite direction of the gradient, gradually reducing the error.
  - The process of forward pass, error calculation, and backward pass is repeated for multiple iterations (epochs) until the model converges or reaches a stopping criterion.
